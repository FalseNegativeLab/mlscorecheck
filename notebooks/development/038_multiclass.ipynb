{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: 3, 1: 5, 2: 4, 3: 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_stratified_folds(classes, n_folds):\n",
    "    folds = []\n",
    "    y = np.hstack([np.repeat(key, value) for key, value in classes.items()])\n",
    "    for _, test in StratifiedKFold(n_splits=n_folds).split(y.reshape(-1, 1), y, y):\n",
    "        folds.append({idx: count for idx, count in enumerate(np.bincount(y[test]))})\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 1, 1: 2, 2: 1, 3: 4}, {0: 1, 1: 2, 2: 1, 3: 3}, {0: 1, 1: 1, 2: 2, 3: 3}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_stratified_folds(classes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_multiclass_fold_to_binary(fold):\n",
    "    n_total = sum(fold.values())\n",
    "    return [{'p': value, 'n': n_total - value} for value in fold.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': 1, 'n': 7}, {'p': 2, 'n': 6}, {'p': 1, 'n': 7}, {'p': 4, 'n': 4}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_multiclass_fold_to_binary(multiclass_stratified_folds(classes, 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiclass_fold(fold, random_state=None):\n",
    "    if not isinstance(random_state, np.random.RandomState):\n",
    "        random_state = np.random.RandomState(random_state)\n",
    "\n",
    "    sample = np.zeros(shape=(len(fold), len(fold)), dtype=int)\n",
    "\n",
    "    for class_idx, count in fold.items():\n",
    "        sample[class_idx, :] = random_state.multinomial(count,\n",
    "                                                        np.ones(len(fold))/len(fold),\n",
    "                                                        size=1)[0]\n",
    "\n",
    "    sums = np.sum(sample, axis=1)\n",
    "    values = np.array(list(fold.values()))\n",
    "\n",
    "    assert np.all(sums == values)\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = multiclass_stratified_folds(classes, 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_fold = sample_multiclass_fold(fold, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlscorecheck.core import safe_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_score_macro(confusion_matrix, score_function):\n",
    "    counts = np.sum(confusion_matrix, axis=1)\n",
    "    n_total = np.sum(counts)\n",
    "\n",
    "    scores = [safe_call(score_function, {'p': counts[idx],\n",
    "                                    'n': n_total - counts[idx],\n",
    "                                    'tp': confusion_matrix[idx, idx],\n",
    "                                    'tn': np.sum(confusion_matrix[0:idx, 0:idx]) + np.sum(confusion_matrix[idx+1:, idx+1:]) +\n",
    "                                            np.sum(confusion_matrix[:idx, idx+1:]) + np.sum(confusion_matrix[idx+1:, :idx])})\n",
    "                for idx, count in enumerate(counts)]\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def multiclass_score_micro(confusion_matrix, score_function):\n",
    "    counts = np.sum(confusion_matrix, axis=1)\n",
    "    n_total = np.sum(counts)\n",
    "\n",
    "    params = {'tp': 0,\n",
    "                'tn': 0,\n",
    "                'p': 0,\n",
    "                'n': 0}\n",
    "\n",
    "    for idx, count in enumerate(counts):\n",
    "        params['p'] += counts[idx]\n",
    "        params['n'] += n_total - counts[idx]\n",
    "        params['tp'] += confusion_matrix[idx, idx]\n",
    "        params['tn'] += np.sum(confusion_matrix[0:idx, 0:idx]) + np.sum(confusion_matrix[idx+1:, idx+1:]) + \\\n",
    "                                            np.sum(confusion_matrix[:idx, idx+1:]) + np.sum(confusion_matrix[idx+1:, :idx])\n",
    "\n",
    "    return safe_call(score_function, params)\n",
    "\n",
    "def multiclass_score_weighted(confusion_matrix, score_function):\n",
    "    counts = np.sum(confusion_matrix, axis=1)\n",
    "    n_total = np.sum(counts)\n",
    "\n",
    "    scores = [safe_call(score_function, {'p': counts[idx],\n",
    "                                    'n': n_total - counts[idx],\n",
    "                                    'tp': confusion_matrix[idx, idx],\n",
    "                                    'tn': np.sum(confusion_matrix[0:idx, 0:idx]) + np.sum(confusion_matrix[idx+1:, idx+1:]) +\n",
    "                                            np.sum(confusion_matrix[:idx, idx+1:]) + np.sum(confusion_matrix[idx+1:, :idx])})\\\n",
    "                * count / n_total\n",
    "                for idx, count in enumerate(counts)]\n",
    "\n",
    "    return np.sum(scores)\n",
    "\n",
    "def multiclass_scores(confusion_matrix, score_function, average):\n",
    "    if average == 'micro':\n",
    "        return multiclass_score_micro(confusion_matrix, score_function)\n",
    "    if average == 'macro':\n",
    "        return multiclass_score_macro(confusion_matrix, score_function)\n",
    "    if average == 'weighted':\n",
    "        return multiclass_score_weighted(confusion_matrix, score_function)\n",
    "\n",
    "    raise ValueError(f'averaging {average} is not supported')\n",
    "\n",
    "def multiclass_accuracy(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, accuracy_standardized, average)\n",
    "\n",
    "def multiclass_sensitivity(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, sensitivity_standardized, average)\n",
    "\n",
    "def multiclass_specificity(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, specificity_standardized, average)\n",
    "\n",
    "def multiclass_balanced_accuracy(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, balanced_accuracy_standardized, average)\n",
    "\n",
    "def multiclass_positive_predictive_value(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, positive_predictive_value_standardized, average)\n",
    "\n",
    "def multiclass_negative_predictive_value(confusion_matrix, average):\n",
    "    return multiclass_score(confusion_matrix, negative_predictive_value_standardized, average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlscorecheck.scores import (\n",
    "    accuracy_standardized,\n",
    "    balanced_accuracy_standardized,\n",
    "    positive_predictive_value_standardized,\n",
    "    sensitivity_standardized,\n",
    "    specificity_standardized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.640625"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_accuracy(confusion_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.random.randint(5, size=20)\n",
    "y_pred = np.random.randint(5, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4041666666666667, 0.25, 0.2638888888888889, None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros(shape=(5, 5))\n",
    "for yt, yp in zip(y_true, y_pred, strict=False):\n",
    "    confusion_matrix[yt, yp] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_sensitivity(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4041666666666667"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_positive_predictive_value(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_accuracy(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_specificity(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_positive_predictive_value(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multiclass(confusion_matrix):\n",
    "    counts = np.sum(confusion_matrix, axis=1)\n",
    "    n_total = np.sum(counts)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for idx, count in enumerate(counts):\n",
    "        p = counts[idx]\n",
    "        n = n_total - counts[idx]\n",
    "        tp = confusion_matrix[idx, idx]\n",
    "        tn = np.sum(confusion_matrix[0:idx, 0:idx]) + np.sum(confusion_matrix[idx+1:, idx+1:]) + np.sum(confusion_matrix[:idx, idx+1:]) + np.sum(confusion_matrix[idx+1:, :idx])\n",
    "        accs.append((tp + tn) / (p + n))\n",
    "\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_multiclass(confusion_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlscorecheck.check import check_1_dataset_known_folds_mos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_folds = transform_multiclass_fold_to_binary(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/gykovacs/anaconda3/envs/mlscorecheck/lib/python3.10/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/a065da42bbc34847a634d09fae57c662-pulp.mps timeMode elapsed branch printingOptions all solution /tmp/a065da42bbc34847a634d09fae57c662-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 7 COLUMNS\n",
      "At line 41 RHS\n",
      "At line 44 BOUNDS\n",
      "At line 54 ENDATA\n",
      "Problem MODEL has 2 rows, 9 columns and 16 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0 - 0.00 seconds\n",
      "Cgl0000I Cut generators found to be infeasible! (or unbounded)\n",
      "Pre-processing says infeasible or unbounded\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inconsistency': True,\n",
       " 'lp_status': 'infeasible',\n",
       " 'lp_configuration': {'evaluations': [{'folds': {'folds': [{'fold': {'p': 1,\n",
       "        'n': 7,\n",
       "        'identifier': 'fkrkv',\n",
       "        'tp': 0.0,\n",
       "        'tn': 7.0},\n",
       "       'scores': {'acc': 0.875},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 2, 'n': 6, 'identifier': 'wpxvc', 'tp': 0.0, 'tn': 6.0},\n",
       "       'scores': {'acc': 0.75},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 1, 'n': 7, 'identifier': 'ngfid', 'tp': 0.0, 'tn': 7.0},\n",
       "       'scores': {'acc': 0.875},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 4,\n",
       "        'n': 4,\n",
       "        'identifier': 'yvtsr',\n",
       "        'tp': 0.0,\n",
       "        'tn': 1.990368},\n",
       "       'scores': {'acc': 0.248796},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True}],\n",
       "     'bounds_flag': True},\n",
       "    'scores': {'acc': 0.687199},\n",
       "    'score_bounds': None,\n",
       "    'bounds_flag': {'folds': [{'fold': {'p': 1,\n",
       "        'n': 7,\n",
       "        'identifier': 'fkrkv',\n",
       "        'tp': 0.0,\n",
       "        'tn': 7.0},\n",
       "       'scores': {'acc': 0.875},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 2, 'n': 6, 'identifier': 'wpxvc', 'tp': 0.0, 'tn': 6.0},\n",
       "       'scores': {'acc': 0.75},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 1, 'n': 7, 'identifier': 'ngfid', 'tp': 0.0, 'tn': 7.0},\n",
       "       'scores': {'acc': 0.875},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True},\n",
       "      {'fold': {'p': 4,\n",
       "        'n': 4,\n",
       "        'identifier': 'yvtsr',\n",
       "        'tp': 0.0,\n",
       "        'tn': 1.990368},\n",
       "       'scores': {'acc': 0.248796},\n",
       "       'score_bounds': None,\n",
       "       'bounds_flag': True}],\n",
       "     'bounds_flag': True}}],\n",
       "  'bounds_flag': True}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_1_dataset_known_folds_mos_scores(dataset={'p': 8, 'n': 24}, folding={'folds': binary_folds}, eps=1e-4, scores={'acc': 0.6873})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlscorecheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
