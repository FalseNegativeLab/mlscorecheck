{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_datasets.binary_classification as binclas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_classifier(random_state):\n",
    "    mode = random_state.randint(4)\n",
    "    if mode == 0:\n",
    "        classifier = RandomForestClassifier\n",
    "        params = {'max_depth': random_state.randint(3, 10),\n",
    "                  'random_state': 5}\n",
    "    if mode == 1:\n",
    "        classifier = DecisionTreeClassifier\n",
    "        params = {'max_depth': random_state.randint(3, 10),\n",
    "                  'random_state': 5}\n",
    "    if mode == 2:\n",
    "        classifier = SVC\n",
    "        params = {'probability': True, 'C': random_state.rand()*2 + 0.001}\n",
    "    if mode == 3:\n",
    "        classifier = KNeighborsClassifier\n",
    "        params = {'n_neighbors': random_state.randint(1, 10)}\n",
    "    \n",
    "    return (classifier, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = binclas.get_filtered_data_loaders(n_col_bounds=(0, 50), n_bounds=(0, 2000), n_minority_bounds=(20, 1000), n_from_phenotypes=1, imbalance_ratio_bounds=(0.2, 20.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [dataset()['name'] for dataset in datasets if not dataset()['name'].startswith('led')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_datasets.binary_classification import summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = summary_pdf[summary_pdf['name'].isin(names)].reset_index(drop=True)\n",
    "tmp = tmp[['name', 'n_col', 'n', 'n_minority', 'imbalance_ratio', 'citation_key']]\n",
    "tmp['name_key'] = tmp.apply(lambda row: f'{row[\"name\"]} \\\\cite{{{row[\"citation_key\"]}}}', axis=1)\n",
    "tmp = tmp[['name_key', 'n', 'n_col', 'n_minority', 'imbalance_ratio']]\n",
    "tmp.columns = ['name', 'size', 'attr.', 'p', 'imb. ratio']\n",
    "tmp['n'] = tmp['size'] - tmp['p']\n",
    "tmp = tmp[['name', 'size', 'attr.', 'p', 'n', 'imb. ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      " & name & size & attr. & p & n & imb. ratio \\\\\n",
      "\\midrule\n",
      "1 & abalone9 18 \\cite{keel} & 731 & 9 & 42 & 689 & 16.40 \\\\\n",
      "2 & appendicitis \\cite{keel} & 106 & 7 & 21 & 85 & 4.05 \\\\\n",
      "3 & australian \\cite{keel} & 690 & 16 & 307 & 383 & 1.25 \\\\\n",
      "4 & bupa \\cite{keel} & 345 & 6 & 145 & 200 & 1.38 \\\\\n",
      "5 & CM1 \\cite{krnn} & 498 & 21 & 49 & 449 & 9.16 \\\\\n",
      "6 & crx \\cite{keel} & 653 & 37 & 296 & 357 & 1.21 \\\\\n",
      "7 & dermatology-6 \\cite{keel} & 358 & 34 & 20 & 338 & 16.90 \\\\\n",
      "8 & ecoli1 \\cite{keel} & 336 & 7 & 77 & 259 & 3.36 \\\\\n",
      "9 & glass0 \\cite{keel} & 214 & 9 & 70 & 144 & 2.06 \\\\\n",
      "10 & haberman \\cite{keel} & 306 & 3 & 81 & 225 & 2.78 \\\\\n",
      "11 & hepatitis \\cite{krnn} & 155 & 19 & 32 & 123 & 3.84 \\\\\n",
      "12 & ionosphere \\cite{keel} & 351 & 33 & 126 & 225 & 1.79 \\\\\n",
      "13 & iris0 \\cite{keel} & 150 & 4 & 50 & 100 & 2.00 \\\\\n",
      "14 & mammographic \\cite{keel} & 830 & 5 & 403 & 427 & 1.06 \\\\\n",
      "15 & monk-2 \\cite{keel} & 432 & 6 & 204 & 228 & 1.12 \\\\\n",
      "16 & new thyroid1 \\cite{keel} & 215 & 5 & 35 & 180 & 5.14 \\\\\n",
      "17 & page-blocks-1-3 vs 4 \\cite{keel} & 472 & 10 & 28 & 444 & 15.86 \\\\\n",
      "18 & PC1 \\cite{krnn} & 1109 & 21 & 77 & 1032 & 13.40 \\\\\n",
      "19 & pima \\cite{keel} & 768 & 8 & 268 & 500 & 1.87 \\\\\n",
      "20 & saheart \\cite{keel} & 462 & 9 & 160 & 302 & 1.89 \\\\\n",
      "21 & shuttle-c0-vs-c4 \\cite{keel} & 1829 & 9 & 123 & 1706 & 13.87 \\\\\n",
      "22 & SPECTF \\cite{krnn} & 267 & 44 & 55 & 212 & 3.85 \\\\\n",
      "23 & vehicle0 \\cite{keel} & 846 & 18 & 199 & 647 & 3.25 \\\\\n",
      "24 & vowel0 \\cite{keel} & 988 & 13 & 90 & 898 & 9.98 \\\\\n",
      "25 & wdbc \\cite{keel} & 569 & 30 & 212 & 357 & 1.68 \\\\\n",
      "26 & wisconsin \\cite{keel} & 683 & 9 & 239 & 444 & 1.86 \\\\\n",
      "27 & yeast1 \\cite{keel} & 1484 & 8 & 429 & 1055 & 2.46 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.index = [idx for idx in range(1, 28)]\n",
    "print(tmp.to_latex(float_format=\"%.2f\").replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "random_state = np.random.RandomState(5)\n",
    "\n",
    "for _ in range(100):\n",
    "    for loader in datasets:\n",
    "        dataset = loader()\n",
    "        X = dataset['data']\n",
    "        y = dataset['target']\n",
    "        name = dataset['name']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "        classifier = generate_random_classifier(random_state)\n",
    "\n",
    "        classifier_obj = classifier[0](**classifier[1])\n",
    "\n",
    "        classifier_obj.fit(X_train, y_train)\n",
    "        y_pred = classifier_obj.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        threshold = random_state.random()\n",
    "\n",
    "        tp = np.sum((y_pred >= threshold) & (y_test == 1))\n",
    "        tn = np.sum((y_pred < threshold) & (y_test == 0))\n",
    "        p = np.sum(y_test)\n",
    "        n = len(y_test) - np.sum(y_test)\n",
    "\n",
    "        acc = np.round((tp + tn) / (p + n), 4)\n",
    "        sens = np.round((tp) / (p), 4)\n",
    "        spec = np.round((tn) / (n), 4)\n",
    "\n",
    "        best_th = -1\n",
    "        best_acc = 0\n",
    "        for th in np.unique(y_pred):\n",
    "            tp = np.sum((y_pred >= th) & (y_test == 1))\n",
    "            tn = np.sum((y_pred < th) & (y_test == 0))\n",
    "            p = np.sum(y_test)\n",
    "            n = len(y_test) - np.sum(y_test)\n",
    "\n",
    "            acc = np.round((tp + tn) / (p + n), 4)\n",
    "            sens = np.round((tp) / (p), 4)\n",
    "            spec = np.round((tn) / (n), 4)\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_th = th\n",
    "\n",
    "        th = best_th\n",
    "\n",
    "        tp = np.sum((y_pred >= th) & (y_test == 1))\n",
    "        tn = np.sum((y_pred < th) & (y_test == 0))\n",
    "        p = np.sum(y_test)\n",
    "        n = len(y_test) - np.sum(y_test)\n",
    "\n",
    "        best_acc = np.round((tp + tn) / (p + n), 4)\n",
    "        best_sens = np.round((tp) / (p), 4)\n",
    "        best_spec = np.round((tn) / (n), 4)\n",
    "\n",
    "        results.append((name, acc, sens, spec, auc, best_acc, best_sens, best_spec, threshold, best_th, p, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results, columns=['dataset', 'acc', 'sens', 'spec', 'auc', 'best_acc', 'best_sens', 'best_spec', 'threshold', 'best_threshold', 'p', 'n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('single.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlscorecheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
